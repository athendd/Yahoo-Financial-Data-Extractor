# https://towardsdatascience.com/text-embeddings-comprehensive-guide-afd97fce8fb5, https://plainenglish.io/blog/how-to-generate-word-embedding-using-bert, https://medium.com/data-science-in-your-pocket/text-summarization-using-textrank-in-nlp-4bce52c5b390
import ntlk
import numpy as np
import pandas as pd
nltk.download('punkt')
import re
from nltk.tokenize import sent_tokenize
from transformers import BertTokenizer
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow as tf
import tensorflow_hub as hub
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import networkx as nx

# cleaning sentences must be done first and foremost

def split_into_tokens(text):
  sentences = nltk.sent_tokenize(text)
  return sentences



df = "link to financial articles on stocks"


# Math behind TextRank

min_threhold = 1e-4

def textrank(matrix, iterations, D):
  N = len(matrix)

  # initialize rank vector for each node and gives all the nodes the same value
  V = [1/N] * N
  V = V / np.linalg.norm(V, 1)

  # rank vector from previous iteration
  Prev_V = 0

  #apply damping factor to matrix
  M_T = (D * M + (1 - D)/N)

  iteration = 0

  # loops through all iterations
  while iteration < iteration:

        # multiplies transition matrix by rank vector to get new ranks for each node
        V = np.dot(M_T, V)

        #if np.linalg.norm(V - Prev_V, 1) < min_threshold: could do it without the one which just sets the axis and calculates euclidean distance between two vectors (could also use manhattan distance)

        # checks for convergence which prevents unnecessary iterations
        if abs(Prev_V - sum(V)) < min_threhold:
            break
        else:
            Prev_V = sum(v)
            iteration += 1
        
    return V

model = SentenceTransformer('all-MiniLM-L6-v2')

n = len(sentences)

# 2d matrix that contains each embedding
similarity_scores_matrix = np.zeros((n, n))

for row in range(n):
  for col in range(n):

    # converts text into their numerical vectors
    row_embedding = model.encode([sentences[row]])
    col_embedding = model.encode([sentences[col]]) 

    # similarity between texts in current row and column
    score = cosine_similarity(row_embedding, col_embedding)

    similarity_scores_matrix[row][col] = score
    similarity_scores_matrix[col][row] = score

# ensures that sum of each row in the matrix equals 1
similarity_score/=np.sum(similarity_score, axis=1)
similarity_score = similarity_score

final_vector = textrank(similarity_score, 400, 0.85)
d = np.argsort(final_vector)

# get sentence ranked from high to low
for i in d.reverse():
    print(text[i])
"""

word_embeddings = {}

# embedds each sentence into a vector
f = open('glove.6B.100d.txt', encoding='utf-8')
for line in f:
  values = line.split()
  word = values[0]
  coefs = np.asarray(values[1:], dtype='float32')
  word_embeddings[word] = coefs
f.close()

sentence_vectors = []
for cleaned_sentence in cleaned_sentences:
  if len(cleaned_sentence) != 0:
    # compute vector for each sentence as the mean of the embeddings
    curr_vector = sum([word_embeddings.get(w, np.zeros(100,))) for w in cleaned_sentence.split()])/(len(cleaned_sentence.split()) + 0.001)
  else:
    curr_vector = np.zeros((100,))
  sentence_vectors.append(curr_vector)

similarity_matrix = np.zeros([len(sentences), len(sentences)])

for i in range(len(sentences)):
  for j in range(len(sentences)):
    if i != j:
      similarity_matrix[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]

#view the graph of the textrank

nx_graph = nx.from_numpy_array(sim_mat)
scores = nx.pagerank(nx_graph)

#get the top N sentences based on their ranking
n = 5
for i in range(n):
  print(ranked_sentences[i][1])
